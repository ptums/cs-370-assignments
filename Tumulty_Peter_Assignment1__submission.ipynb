{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Initial\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Stochastic Gradient Descent (SGD).\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Increased hidden neurons to 200\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Stochastic Gradient Descent (SGD).\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 200\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Decreased hidden neurons to 75\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Stochastic Gradient Descent (SGD).\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 200\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Increased hidden neurons to 200 with additional layer\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Stochastic Gradient Descent (SGD).\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 200\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Run vs. Updated Run\n",
    "\n",
    "## Original Run Details\n",
    "\n",
    "The initial neural network script results after the training we achieve about 98.6% accuracy on training, 97.2% on validation, 97.4% on the test, and it took 21.76 minutes to complete the training.\n",
    "\n",
    "<img src=\"./2originalrun.png\" alt=\"original run 2\"  width=\"600\"/>\n",
    "\n",
    "## Updated Run Details\n",
    "\n",
    "After making changes to neural network by adding another hidden layer, changing the optimizer algorithm, and a drop out for each hidden layer results after the training we achieve about 98.1% accuracy on training, 97.8% on validation, 97.9% on the test, and it took 2.6 minutes to complete the training.\n",
    "\n",
    "<img src=\"./2updatedrun.png\" alt=\"updated run 2\"  width=\"600\"/>\n",
    "\n",
    "## What happens to the accuracy rates for the training, validation, and test data sets as you change the parameters?\n",
    "\n",
    "### Epochs\n",
    "The way neural networks train themselves on a set of data is by constantly being exposed to the data and making sense of it. The constant exposure is broken down into rounds or cycles. The epoch is the number of cycles you want our neural network model to be exposed to the data before its training its complete. Initially, our neural network had a very basic configuration so making the epoch number high made sense if we wanted to achieve higher accuracy on our results. The drawback of having a high number of epochs it takes longer for the neural network to complete its training. As you can see when our `NB_EPOCH = 200` it took 21.76 minutes to complete the training. So even though we have a 98.6% accuracy on training, 97.2% on validation, 97.4% on the test it look a substantialy amount of time to achieve this goal.\n",
    "\n",
    "In our second round we lowered `NP_EPOCH = 20` and if that is all we changed the training time would drastically decrease, but the training goals would be far from accurate. For example, adjusting the epoch to 20 and changing nothign else about the model script resulted in 93% accuracy on training, 93.4% on validation, 93.4% on the test, and it took 2.3 minutes to complete the training. The time it took to complete the training decreased, which is good. However, the rest the training metrics decreased as well, which is not good. The objective is to obtain our training goals and minimize the time it takes to acheive these goals. So we are going to set the epoch to 20, we have to make further optimizations to our neural network.\n",
    "\n",
    "### Optimizers\n",
    "The purpose of selecting an optimizer algorithm is to update the weights while the model goes through each round of training. Selecting an optimizer algorithm also requires an objective function (loss functions), because the algorithm will excute this function in order to adjust the weights and biases to minimize any loss between the predicted and actual output. The initial optimizer algorithm used for the neural network was a stochastic gradient descent `OPTIMIZER = SGD()`. The way this gradient descent works is it performs one weight update every epoch. Though very efficient, it requires many more runs because it also generates a lot of noise, which the model must interpret.\n",
    "\n",
    "When we updated the model and lowered the number of epochs we introducted a more advanced optimization algorithm known as Adam `OPTIMIZER = ADAM()`. This algorithm is similar to SGD interms of acceleration, but also includes another concept of momentum as well in the form of velocity. Using Adam allows for faster convergence at the cost of more computation. Since Adam has a more complex design than SGD if we keep the epoch number high and used Adam as the optimization algorithm it would take several hours to complete the training. Therefore, if it the model requires a more advanced optimization algoirthm having a high number of epochs is unncessary to achieve the training goals.\n",
    "\n",
    "\n",
    "### Additional Hidden Layer\n",
    "What is a hidden layer, their purpose in the neural network and how is it implemented? Also, what is the pros/cons of adding an additional layer.\n",
    "\n",
    "### Dropout \n",
    "\n",
    "What is a dropout, their purpose in the neural network and how is it implemented? Also, what is the pros/cons of adding an additional layer.\n",
    "\n",
    "## Conclusion\n",
    "Share why adding the additional changes made the neural network more accurate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
