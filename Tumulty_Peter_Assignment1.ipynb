{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Initial\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Adam\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Increased hidden neurons to 200\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Adam\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 200\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Decreased hidden neurons to 75\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Adam\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 75\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Increased hidden neurons to 200 with additional layer\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Adam\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 200\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# third hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Increased hidden neurons to 200 with SGD optimization algorithm\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# add this to see the difference in time it took to complete\n",
    "import time\n",
    "\n",
    "\n",
    "# parameters - original \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 40\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "\n",
    "#  Set the optimization algorithm used to train the model's weights to Stochastic Gradient Descent (SGD).\n",
    "#  This impacts how the neural network learns\n",
    "OPTIMIZER = Adam() \n",
    "N_HIDDEN = 200\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT=0.3\n",
    "\n",
    "# setting up the pixels of the image with a label \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "# transforms the image data from MNIST dataset\n",
    "# to a data set that can be used by keras\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# continues to transform the test image data into pixel values.\n",
    "# by dividing every pixel by 255 so it can be a number either between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# Converting the label data from MNIST dataset into a format\n",
    "# that can be used by keras\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# set up model\n",
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# second hidden layer \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# We set how the optimizer algorithm and how the model\n",
    "# should handle loss between runs and what metric we are interested in\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# start timer \n",
    "start_time = time.time()\n",
    "\n",
    "# We configure the model to use training data, batch size, number of epochs etc.\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "final_time = end_time - start_time\n",
    "\n",
    "# contains the results of the training\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(f\"Test completion time: {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments \n",
    "\n",
    "- Initial \n",
    "- Increased hidden neurons to 200\n",
    "- Decreases hidden neurons to 75\n",
    "- Increased hidden neurons to 200 with an additional hidden layer\n",
    "- Increased hidden neurons to 200 with SGD optimization algorithm\n",
    "\n",
    "# Results \n",
    "\n",
    "|                                                              | Accuracy | Validation | Test | Time (minutes) |\n",
    "|--------------------------------------------------------------|----------|------------|------|------|\n",
    "| Initial                                                      |    98.1%      |  97.8%  | 97.9%       |      11.7     |\n",
    "| Increased hidden neurons to 200                              |     98.6%     |    98%        |  98%    |   19   |\n",
    "| Decreased hidden neurons to 75                               |       96.6%   |       97.3%     |   97.4%   |  7.1    |\n",
    "| Increased hidden neurons to 200 with additional hidden layer |       98.4%   |     97.8%       |   98%   |   14.58   |\n",
    "| Increased hidden neurons to 200 with SGD algorithm |       95.9%   |     96.9%       |   96.7%   |   8.1   |\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In my experiment, I worked with the `N_HIDDEN` parameter, representing the hidden neurons in the code. Initially, I set the hidden neurons to 128, following the guidance from Chapter 1, \"Neural Network Foundations,\" in \"Deep Learning with Keras.\" I used the code from this chapter for the neural network but made minor adjustments to include a variable measuring the completion time `final_time` for each test. I believed this data would clarify the performance of each experiment, especially since the accuracy, validation, and test results were closely matched.\n",
    "\n",
    "My hypothesis was that increasing the hidden neurons would lead to higher accuracy, validation, and testing percentages but would also extend the training time. This proved correct, as we achieved 98% for accuracy, validation, and testing, and the training took 19 minutes to complete. Compared to the initial run and the run with fewer hidden neurons, this training took much longer. Since our results in terms of accuracy, validation, and testing were really good, I wanted to optimize our model further. The goal was to maintain 200 hidden neurons, keep the results at 98%, and reduce the training time.\n",
    "\n",
    "I set up two experiments to try and reduce the training time. The first experiment was to add an additional layer. I thought that having another hidden layer would make managing the complexity of having such a high number of hidden neurons per epoch easier, so the model could quickly interpret the data. This approach did reduce the training time. However, it also lowered the results for accuracy, validation, and testing. So overall, this was not an ideal optimization.\n",
    "\n",
    "The second experiment I tried was changing the optimization algorithm. In the initial script, we changed the algorithm from SGD to Adam because Adam is more complex than SGD, so it should theoretically train better on the data. However, I suspected that running a more complex algorithm to handle the loss between runs might be slowing things down. So, I tried using SGD, which has a more straightforward impact, hoping the results would remain the same and the training time would decrease. This hypothesis also turned out to be incorrect; the training time decreased, but the results for the other metrics were also lower.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
