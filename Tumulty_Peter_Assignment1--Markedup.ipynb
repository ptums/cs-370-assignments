{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Assignment tasks:\n",
    "    1. Notebook configuration and naming convention -X\n",
    "    2. Establishes Accuracy Scores with Two Hidden Layers -X (need snap shot)\n",
    "    3. Establishes Accuracy Scroes by varying parameters\n",
    "    4. Explain changes in accuracy rats \n",
    "    5. Articuation of Response \n",
    "\n",
    "'''\n",
    "\n",
    "# make sure the print statement in the code works the same way in both older and newer versions of Python.\n",
    "from __future__ import print_function\n",
    "# NumPy is used for working with arrays and matrices of numerical data. Provides tools to create,\n",
    "# manipulate, and perform calculations on large collections of numbers efficiently. \n",
    "import numpy as np\n",
    "\n",
    "# MNIST stands for Modified National Institute of Standards and Technology,\n",
    "# and it is a widely-used dataset in the field of machine learning and computer vision.\n",
    "# The MNIST dataset consists of 70,000 small images of handwritten digits (from 0 to 9),\n",
    "#along with their corresponding labels (the actual digit that the image represents).\n",
    "from keras.datasets import mnist\n",
    "\n",
    "'''\n",
    "Those lines of code are importing different components from the Keras library\n",
    "that allow you to build and set up a neural network model for machine learning tasks like image classification.\n",
    "- Sequential helps define the neural network architecture as a sequence of layers.\n",
    "- Dense and Activation are two types of layers you can add to the network\n",
    "    -- Dense layers are for computing weighted sums of inputs,\n",
    "- Activation layers apply an activation function like ReLU.\n",
    "- SGD is an optimization algorithm that adjusts the network's weights during training.\n",
    "- np_utils provides some utilities for working with NumPy arrays which are often used to represent data inputs and labels.\n",
    "'''\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "'''\n",
    "This code adds an additional dense layer to a neural network model and applies L2\n",
    "regularization to the weights of that layer. Specifically, it imports the\n",
    "regularizers module from the Keras library, which provides different techniques to prevent overfitting.\n",
    "It then adds a dense layer with 64 nodes, where the input dimension is also 64. The\n",
    "kernel_regularizer=regularizers.l2(0.01) part tells the layer to apply L2 regularization to\n",
    "the weights connecting the inputs to the nodes in this layer. L2 regularization adds a penalty\n",
    "term to the loss function that discourages the weights from becoming too large during training. This\n",
    "encourages the model to learn smaller weights, making it less likely to overfit to the training data.\n",
    "The 0.01 value determines the strength of the regularization penalty - larger values increase the \n",
    "regularization effect. Adding this regularized layer helps the model generalize better to unseen data.\n",
    "\n",
    "** 5 year old explanation\n",
    "\n",
    "Imagine you're building a really cool tower with your building blocks. This tower is special because it can look at pictures of\n",
    "numbers and tell you what number it is. Pretty neat, right?\n",
    "\n",
    "Now, when you're building your tower, you want to make sure it doesn't get too big and wobbly. If you make it too\n",
    "tall and unstable, it might fall over and not work properly anymore.\n",
    "\n",
    "That's kind of like what can happen when we're training our special number-guessing tower (the neural network model). If we\n",
    "let the tower get too big and complicated, it might start learning things that aren't really\n",
    "important, and it won't be good at guessing new numbers it hasn't seen before.\n",
    "\n",
    "So, these lines of code are like adding a special rule when we're building our tower.\n",
    "The rule says that whenever we add a new layer of blocks (the dense layer with 64 nodes),\n",
    "we have to be careful not to make it too big or heavy.\n",
    "\n",
    "The kernel_regularizer=regularizers.l2(0.01) part is like telling the tower,\n",
    "\"Hey, when you're adding these new blocks, make sure you don't use too many of them, or \n",
    "use you'll get a little penalty.\" The 0.01 part is like saying how strict we want to be with\n",
    "this rule â€“ a bigger number would mean we're super strict, and a smaller number means we're a little more relaxed.\n",
    "\n",
    "By adding this rule, we're making sure our tower doesn't get too big and wobbly. It helps our tower stay nice and stable, so it\n",
    "can keep guessing numbers correctly, even when it sees new pictures it hasn't seen before.\n",
    "\n",
    "\n",
    "'''\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "\n",
    "# sets the starting point for generating random numbers in NumPy to a specific value (1671),\n",
    "np.random.seed(1671) \n",
    "\n",
    "'''\n",
    "Sets various configuration options and *hyperparameters for a machine learning model that will be trained.\n",
    "- NB_EPOCH=20 means the model will be trained across 20 iterations of the entire dataset.\n",
    "- BATCH_SIZE=128 specifies that the data will be divided into batches of 128 examples when training the model.\n",
    "- VERBOSE=1 controls how much information is printed to the screen during training.\n",
    "- NB_CLASSES=10 indicates there are 10 possible *output classes (likely the digits 0-9).\n",
    "- OPTIMIZER=SGD() sets the optimization algorithm used to *train the model's weights to Stochastic Gradient Descent.\n",
    "- N_HIDDEN=128 defines the number of nodes in a *hidden layer of the model to be 128. \n",
    "- VALIDATION_SPLIT=0.2 reserves *20% of the training data to be used for validation and monitoring the model's performance during training.\n",
    "'''\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 \n",
    "\n",
    "'''\n",
    "The difference between SGD & ADAM\n",
    "'''\n",
    "OPTIMIZER = Adam()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2\n",
    "DROPOUT=0.3\n",
    "'''\n",
    "Those lines are loading the MNIST dataset of handwritten digit images and\n",
    "splitting it into training and testing sets. The mnist.load_data() function returns two tuples\n",
    "- one for the training data\n",
    "- one for the test data.\n",
    "Each tuple contains two elements:\n",
    "- the images themselves (X_train and X_test)\n",
    "- their corresponding labels (y_train and y_test).\n",
    "The images are being unpacked into X_train and X_test,while the labels are unpacked\n",
    "into y_train and y_test. This allows the code to have separate image and label data for\n",
    "training a machine learning model and evaluating its performance on unseen test data.\n",
    "The RESHAPED = 784 line is likely indicating that each 28x28 pixel image will be reshaped\n",
    "or flattened into a 1-dimensional vector of length 784 (28 * 28) for use as input to the model.\n",
    "'''\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "'''\n",
    "Theses lines of code are pre-processing the image data from the MNIST dataset to prepare it for use in training\n",
    "and testing a machine learning model.\n",
    "- the first two lines reshape the training and test image data from their original 2D shape (28x28 pixels) into 1D vectors of length 784 (28*28).\n",
    "This flattening step is necessary because many machine learning models require input data to be 1-dimensional.\n",
    "- The next two lines convert the data type of the training and test images from integers to 32-bit floating-point numbers.\n",
    "This is often required because many machine learning algorithms expect input data to be floating-point values, not integers.\n",
    "By reshaping the 2D image arrays into 1D vectors and converting data types, this code is transforming the raw image data into a\n",
    "format that can be efficiently processed by the machine learning model in the subsequent steps.\n",
    "\n",
    "'''\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "'''\n",
    "Those lines are performing additional preprocessing steps on the training and test image\n",
    "data to standardize the pixel values. The first two lines divide every pixel value\n",
    "in the training and test images by 255. This scales all the pixel values, which originally\n",
    "ranged from 0 to 255, to be between 0 and 1.\n",
    "\n",
    "Scaling input data is a common practice in machine learning to ensure differentfeatures are\n",
    "on a similar scale. The next two lines simply print out the number of training and test\n",
    "samples in the dataset. This provides a sanity check to verify the dataset has been loaded\n",
    "and preprocessed correctly before proceeding to train the model. Specifically, it will\n",
    "print something like \"60000 train samples\" and \"10000 test samples\" since the MNIST dataset\n",
    "contains 60,000 training and 10,000 test images originally.\n",
    "'''\n",
    "X_train /= 255 \n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "'''\n",
    "Those lines are converting the label data (y_train and y_test) from their original integer format into a\n",
    "categorical format suitable for training a *multi-class classification model. Initially,\n",
    "the labels are represented as single digits (0 to 9) indicating which handwritten digit the image\n",
    "represents. However, many machine learning models expect the labels to be *\"one-hot\" encoded vectors instead\n",
    "of integers. The np_utils.to_categorical function from Keras does this encoding automatically. It takes the\n",
    "integer labels and the total number of classes (NB_CLASSES=10 for digits 0-9), and converts each label\n",
    "into a vector with 0s in all positions except a 1 in the index corresponding to that digit class. This\n",
    "allows the model to interpret the output as a probability distribution over the 10 possible classes\n",
    "during training and prediction. So y_train and y_test now contain the categorically encoded label data\n",
    "matching the format expected by the model.\n",
    "\n",
    "** 5 year old version \n",
    "\n",
    "You know how when you're learning your numbers, your teacher might give you a worksheet with pictures\n",
    "of numbers and ask you to circle or color in the correct number?\n",
    "\n",
    "Well, imagine that instead of just circling the number, you had to color in a whole row of circles, and\n",
    "the circle that matches the number in the picture is the only one you can color in.\n",
    "\n",
    "For example, if the picture shows the number 3, you'd have a row of 10 circles (one for each number from 0 to 9), and you'd color in\n",
    "the 3rd circle, leaving the rest blank.\n",
    "\n",
    "That's kind of what these lines of code are doing with the labels (the actual numbers that match each picture).\n",
    "\n",
    "Originally, the labels are just single numbers (like 3 or 7). But the computer program we're building\n",
    "needs the labels in a special format where each number is represented by a row of circles, with only one circle colored in.\n",
    "\n",
    "So these lines of code take the original labels (like 3 or 7) and convert them into rows of\n",
    "10 circles, with only the circle matching the number colored in.\n",
    "\n",
    "This special format with rows of circles (called \"one-hot encoding\") is what the computer program expects,\n",
    "so it can learn to match the pictures of numbers with the correct row of circles (the correct label).\n",
    "'''\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "'''\n",
    "That code is defining the architecture and compiling a neural network model\n",
    "for the task of classifying handwritten digit images. It first creates a Sequential model,\n",
    "which is a linear stack of layers. It adds a Dense layer with N_HIDDEN (128) nodes as the input\n",
    "layer, followed by a ReLU activation function. It then adds another Dense layer with N_HIDDEN nodes,\n",
    "another ReLU activation, a Dense output layer with NB_CLASSES (10) nodes for the 10 digit classes, and finally\n",
    "a softmax activation to output class probabilities. The model.summary() prints a summary of the model architecture. \n",
    "inally, it compiles the model by specifying the loss function as categorical cross-entropy (for multi-class problems),\n",
    "the previously set optimizer SGD, and accuracy as the evaluation metric. This compiled model is now ready\n",
    "to be trained on the preprocessed image data.\n",
    "\n",
    "** 5 year old version\n",
    "Imagine you have a big box of building blocks, and you want to build a really cool tower with them.\n",
    "But not just any tower - you want to build one that can look at pictures of numbers and tell you what number it is!\n",
    "\n",
    "First, we need to get all the blocks ready. The line model = Sequential() is like saying, \"I'm going to build my tower one block at a time,\n",
    "stacking them up in a line.\"\n",
    "\n",
    "Then, we start adding different kinds of blocks to the tower. The lines like model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) and model.add(Activation('relu'))\n",
    "are adding special blocks that are really good at looking at the pictures of numbers and finding patterns.\n",
    "\n",
    "Some of the blocks are called \"Dense\" blocks, and they're like the bricks that make up the main part of the tower. The \"Activation\" blocks\n",
    "are like little helpers that make sure the tower is working properly.\n",
    "\n",
    "We keep adding more and more of these special blocks, until we have a really tall tower\n",
    "with lots of different kinds of blocks. The last few blocks, like model.add(Dense(NB_CLASSES)) and model.add(Activation('softmax')), are the ones that\n",
    "actually look at the picture of the number and tell us what number it is.\n",
    "\n",
    "Finally, we have the line model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy']). This is like giving the tower some\n",
    "instructions on how to learn and get better at recognizing the numbers. It's like telling the tower, \"When you see a picture of a\n",
    "number, try your best to guess what it is. If you get it wrong, don't worry, just learn from your mistakes and keep practicing!\"\n",
    "\n",
    "So, with all these special blocks and instructions, our tower is now ready to start learning how to recognize\n",
    "numbers in pictures. It's like a really cool, number-guessing tower that we built ourselves!\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layer 1\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer 2 \n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "\n",
    "'''\n",
    "The purpose of adding dropout \n",
    "\n",
    "Dropout is a technique used to prevent overfitting in neural networks during training.\n",
    "Overfitting happens when the model learns the training data too well, including the noise and\n",
    "irregularities, causing it to perform poorly on new unseen data. The line model.add(Dropout(DROPOUT))\n",
    "is adding a dropout layer to the model architecture. This layer randomly drops out (sets to zero) a fraction (0.3 or 30% based on DROPOUT=0.3)\n",
    "of the nodes in that layer during each training iteration. This helps prevent nodes from becoming too reliant on the\n",
    "presence of other nodes, forcing them to learn more robust features. By randomly dropping nodes, different nodes get exposed\n",
    "to different data, making the overall network more generalizable. After training, dropout is disabled, allowing the full network\n",
    "to make predictions on new data. Incorporating dropout makes the model more robust, reducing overfitting on the training data, and\n",
    "improving its ability to generalize well on unseen data.\n",
    "'''\n",
    "\n",
    "# NEW STUFF\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "\n",
    "'''\n",
    "This code is training the previously defined neural network model on the handwritten digit\n",
    "image data, evaluating its performance on the test set, and printing the test metrics.\n",
    "- model.fit() is the function that trains the model by showing it the training images (X_train) and labels\n",
    "(Y_train) -- VISUAL \n",
    "- for a set number of iterations (epochs=20) in small batches (batch_size=128). - KINDA HAS TO DO WITH ROUNDS...\n",
    "The validation_split reserves 20% of the training data for monitoring the model's performance during training. - I get that, but why?\n",
    "After training, model.evaluate() computes the loss and accuracy of the trained model on the test images\n",
    "(X_test) and labels (Y_test). Finally, it prints out the test loss (score[0]) and test accuracy (score[1]) scores,\n",
    "allowing you to assess how well the model performed on data it has not seen before.\n",
    "\n",
    "'''\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
